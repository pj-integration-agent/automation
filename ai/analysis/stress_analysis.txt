Relatório Técnico – Teste de Stress – JMeter (JTL Resumido)

1. Resumo do Cenário de Teste  
O teste foi executado com um único thread (“GET PokéAPI - CI 1‑x”) em 20 requisições GET à URL https://pokeapi.co/api/v2/pokemon/ditto. Cada requisição retornou código 200 OK. O objetivo era validar a resposta e medir latências em um cenário de baixa concorrência.

2. Volume Total de Requisições  
20 solicitações foram registradas no período analisado.

3. Throughput Estimado  
O intervalo total do teste foi 9 420 ms (9,42 s).  
Throughput = 20 req / 9,42 s ≈ 2,12 req/s.  
Como o teste utilizou apenas uma thread, o throughput não reflete a capacidade real do servidor sob carga concorrente.

4. Latência  
Valores de latência (ms) observados: 331, 32, 27, 68, 26, 25, 23, 23, 30, 30, 46, 20, 24, 27, 31, 44, 30, 23, 24, 21.  
Percentis calculados:

* p50 (mediana) ≈ 27 ms  
* p90 ≈ 46 ms  
* p95 ≈ 318 ms  
* p99 ≈ 331 ms  

O p95 e p99 são dominados por um outlier de 331 ms, indicando possível problema de rede ou gargalo esporádico.

5. Taxa de Erros  
Todas as requisições foram marcadas como sucesso (“success = true”).  
Taxa de erros = 0 % (0 de 20).

6. Principais Gargalos Identificados  
* Latência alta (331 ms) em uma única requisição, sugerindo falha transitória ou limite de taxa no endpoint.  
* Variedade de latências (p90 a p95) indica variabilidade de rede ou do serviço externo.  
* Throughput limitado pela ausência de concorrência; não há evidência de gargalos internos ao servidor alvo, mas a configuração do teste não permite avaliação de escalabilidade.

7. Impacto Potencial em Ambiente de Produção  
Com um único usuário, o teste não simula a carga real que a aplicação pode receber. Se a aplicação depender de múltiplos clientes simultâneos, o desempenho pode degradar, especialmente se existir limitação de conexões ou throttling da API externa. O outlier de 331 ms pode afetar a experiência do usuário em picos de tráfego.

8. Recomendações Técnicas  
* Incrementar o número de threads para representar a carga esperada (por exemplo, 50–200 usuários simultâneos) e medir throughput e latência sob concorrência.  
* Utilizar Constant Throughput Timer ou Targeted Load para garantir que a taxa de requisições atinja a meta de produção.  
* Monitorar métricas de servidor (CPU, memória, throughput de rede) e métricas da API externa (tamanho de cache, limites de taxa) para identificar gargalos de recurso.  
* Analisar o outlier de 331 ms: revisar logs do endpoint, verificar tempo de conexão (coluna “Connect”) e considerar a aplicação de timeouts mais rigorosos.  
* Configurar pool de conexões HTTP no JMeter (HTTP Request Defaults) para reutilizar conexões, reduzir overhead de handshake e melhorar throughput.  
* Avaliar a possibilidade de usar testes distribuídos (Distributed JMeter) para escalar além da máquina local e validar desempenho em escala maior.  
* Documentar resultados detalhados, incluindo percentis de latência, para comparação entre versões de código ou infraestrutura.  

Este relatório fornece uma visão inicial do comportamento sob carga mínima. Para uma avaliação completa de escalabilidade e resiliência, recomenda-se executar testes adicionais com cenários de concorrência realistas e monitoramento de métricas de back‑end.