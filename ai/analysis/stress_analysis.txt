Relatório Técnico – Teste de Stress (Apache JMeter)

Resumo do Cenário de Teste
O teste foi executado com duas threads (GRPTHREADS=2 na primeira execução e GRPTHREADS=1 nas demais) durante aproximadamente 9,4 segundos, resultando em 20 requisições GET para a URL https://pokeapi.co/api/v2/pokemon/ditto. Todas as respostas retornaram código HTTP 200 OK. A carga aplicada corresponde a um cenário de baixa concorrência, típico de testes de verificação de estabilidade inicial.

Volume Total de Requisições
O arquivo JTL contém 20 registros de requisição, correspondendo a 20 chamadas ao endpoint de Pokémon.

Throughput Estimado
Tempo total entre a primeira e a última requisição = 9,401 ms.  
Throughput = 20 requisições / 9,401 ms ≈ 2,13 requisições por segundo (RPS).  
Este throughput reflete apenas a carga aplicada no teste atual; não representa o limite de capacidade do servidor.

Latência (Percentis)
Latência (ms) – valores coletados: 684, 280, 57, 57, 58, 54, 54, 51, 51, 52, 45, 53, 71, 49, 47, 54, 50, 51, 50, 50.  
- p50 (mediana) ≈ 51,5 ms  
- p90 ≈ 58 ms  
- p95 ≈ 71 ms  
- p99 ≈ 684 ms  

O percentil p99 revela um pico de latência excepcionalmente alto (684 ms), enquanto a maioria das requisições permanece abaixo de 60 ms.

Taxa de Erros
Todas as 20 requisições tiveram sucesso (SUCCESS = TRUE).  
Taxa de erros = 0 %.  

Principais Gargalos Identificados
1. Latência elevada em requisições iniciais (684 ms e 280 ms) – indica possível latência de estabelecimento de conexão ou cache não populado.  
2. Conexão única por thread (Connect = 656 ms na primeira, 254 ms na segunda, 37 ms nas demais) sugere que o Keep-Alive não foi reutilizado entre requisições subsequentes, gerando overhead de TCP handshake.  
3. Baixa concorrência (2 threads) limita a capacidade de avaliar a escalabilidade do serviço sob carga pesada.

Impacto Potencial em Ambiente de Produção
- A latência média abaixo de 60 ms é aceitável para a maioria de aplicações, mas o pico de 684 ms pode degradar a experiência do usuário em cenários de tráfego alto.  
- Sem erros, a aplicação demonstra robustez básica; contudo, a baixa taxa de requisições simultâneas não revela problemas de escalabilidade.  
- Se a produção for exposta a dezenas ou centenas de usuários simultâneos, o tempo de estabelecimento de conexão pode aumentar, elevando a latência total e potencialmente atingindo limites de recursos do servidor (CPU, memória, conexões de rede).

Recomendações Técnicas
1. Habilite Keep-Alive nas configurações do cliente JMeter para reutilizar conexões TCP, reduzindo o overhead de handshake e a latência de conexão.  
2. Configure o servidor para aceitar conexões persistentes e aumentar o número máximo de conexões simultâneas, caso ainda não esteja otimizado.  
3. Realize testes com maior número de threads (ex.: 50–100) e aumente a duração do teste para avaliar a resposta do sistema sob carga contínua e identificar gargalos de CPU ou memória.  
4. Monitore métricas de sistema (CPU, memória, I/O, utilização de sockets) durante os testes para correlacionar picos de latência com recursos saturados.  
5. Considere o uso de um pool de conexões na aplicação back-end para reduzir o custo de criação de novas conexões a recursos externos ou bases de dados.  
6. Analise logs de aplicação e rede para identificar possíveis timeouts ou retransmissões que possam contribuir para a latência observada.  
7. Caso o servidor esteja balanceado em múltiplos nós, verifique a distribuição de carga e a configuração do load balancer para evitar hotspots.  
8. Teste a aplicação em ambientes de rede semelhantes ao de produção (latência de rede, largura de banda) para validar a real performance.  
9. Se o pico de latência persistir, investigue o código que processa a requisição GET /pokemon/ditto – pode haver chamadas externas, cache frio ou cálculo intensivo que não aparece em testes de baixa carga.  

Conclusão
O teste atual demonstra que o serviço pode atender a cerca de 2,1 requisições por segundo com latência média aceitável, mas apresenta picos de latência associados a estabelecimento de conexão. Para garantir performance em produção, recomenda‑se otimizar o uso de Keep-Alive, aumentar a carga de teste e monitorar recursos de sistema. Implementar as medidas acima reduzirá latência, evitará gargalos e preparará o ambiente para escalabilidade real.