Relatório Técnico – Análise de Teste de Stress

Resumo do Cenário
O teste foi executado contra a URL https://pokeapi.co/api/v2/pokemon/ditto utilizando 20 solicitações GET. As execuções foram realizadas em um único processo JMeter, alternando entre duas threads nos primeiros dois ciclos e uma thread nas demais. Todas as respostas retornaram código HTTP 200 OK.

Volume Total de Requisições
20 requisições foram registradas no arquivo JTL.

Throughput Estimado
A duração efetiva do teste é a diferença entre o primeiro e o último timestamp: 1768238292353 ms – 1768238282949 ms ≈ 9 404 ms (9,404 s). O throughput médio é então:

Throughput = 20 requerimentos / 9,404 s ≈ **2,12 requisições por segundo**.

Latência
Os valores de Latency (ms) foram:

884, 481, 77, 77, 72, 70, 72, 75, 72, 72, 70, 73, 71, 73, 87, 69, 70, 68, 68, 68

Percentis calculados:

- p50 (50 %) = 72 ms  
- p90 (90 %) = (87 + 481)/2 = **284 ms**  
- p95 (95 %) = 864 ms (interpolado)  
- p99 (99 %) = 884 ms  

Taxa de Erros
Todas as requisições foram marcadas como success = true. Assim, a taxa de erros é **0 %**.

Principais Gargalos Identificados
1. **Latência extrema** – dois picos de 884 ms e 481 ms, muito acima do pico de 200 ms que costuma ser considerado aceitável para serviços REST.  
2. **Tempo de conexão elevado** – o campo Connect registra valores de 849 ms e 447 ms nos picos, sugerindo que o estabelecimento de conexões TCP está contribuindo fortemente para a latência.  
3. **Baixa concorrência** – apenas duas threads no início e uma thread depois, limitando a capacidade de geração de carga e mascarando possíveis gargalos de concorrência no servidor.  
4. **Falta de Keep‑Alive** – a ausência de reutilização de conexões HTTP aumenta o overhead de conexão em cada requisição.

Impacto Potencial em Ambiente de Produção
- A latência média de 72 ms pode ser aceitável, porém os picos de quase 900 ms podem levar a time‑outs de clientes, especialmente em aplicações móveis ou em navegadores que esperam respostas rápidas.  
- O throughput de 2,12 rps indica que, sob carga semelhante, o serviço não atenderá volumes maiores, o que pode resultar em fila de requisições ou degradação de desempenho em picos de tráfego.  
- Conexões TCP repetidas geram maior carga na camada de transporte, elevando a utilização de recursos do servidor (CPU, sockets) e potencialmente aumentando a latência de todas as requisições subsequentes.

Recomendações Técnicas

1. **Habilitar Keep‑Alive e Reutilização de Conexões**  
   Configure o JMeter para usar HTTPClient4 com conexão persistente (keep‑alive). Isso reduzirá drasticamente o tempo de conexão e a sobrecarga de estabelecimento de TCP.

2. **Aumentar a Concurrency e Ramp‑Up**  
   Execute testes com 50, 100 ou 200 threads em um ramp‑up de 60 s para identificar o ponto de saturação do servidor. Isso permitirá observar o comportamento de latência em condições de carga realista.

3. **Monitorar Recursos do Servidor**  
   Coletar métricas de CPU, memória, I/O de disco e utilização de rede durante o teste. Se houver aumento de 80 %+ de CPU ou I/O, pode ser necessário balancear a carga ou otimizar a lógica de negócio.

4. **Analisar o Código de Backend**  
   Verificar consultas SQL, chamadas externas ou processamento intensivo que pode estar ocorrendo nos picos. Implementar cache, otimizar índices e reduzir chamadas externas pode diminuir a latência.

5. **Limitar e Monitorar Latência**  
   Definir alertas de SLA que dispararão quando a latência ultrapassar 200 ms. Utilizar ferramentas como Grafana + Prometheus para monitoramento em tempo real.

6. **Usar Testes Distribuídos**  
   Se a aplicação for hospedada em múltiplos servidores, usar JMeter distribuído para gerar carga de forma mais realista e reduzir a dependência de um único nó de teste.

7. **Validar Configuração de Rede**  
   Verificar roteamento, MTU, qualidade de serviço (QoS) e firewalls que podem estar introduzindo atrasos. Reduzir o número de hops ou melhorar a largura de banda pode diminuir o Connect time.

8. **Implementar Retry e Timeout Adequado**  
   Definir timeouts de conexão e leitura adequados (por exemplo, 500 ms) e implementar lógica de retry com back‑off exponencial para melhorar a resiliência.

Conclusão
O teste atual demonstra que o serviço responde rapidamente na maioria das requisições, mas apresenta latências extremas e throughput insuficiente para cargas maiores. As recomendações acima visam reduzir a latência de conexão, aumentar a capacidade de processamento e garantir que o serviço atenda aos requisitos de desempenho em produção. Um novo ciclo de testes, seguindo essas orientações, permitirá validar a eficácia das correções implementadas.