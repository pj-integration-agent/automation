Resumo do cenário de teste  
O teste foi executado em ambiente de Integração Contínua (CI) utilizando o Apache JMeter. O objetivo era avaliar a capacidade de resposta da API externa PokéAPI ao receber requisições GET do endpoint /pokemon/ditto. Foram configuradas duas threads simultâneas, cada uma executando 10 requisições, totalizando 20 chamadas HTTP. O teste foi curto, com duração de aproximadamente 9 segundos, e não houve falhas de conexão nem de resposta HTTP (código 200) em nenhuma das 20 solicitações.

Volume total de requisições  
O conjunto de dados contém exatamente 20 requisições.

Throughput estimado  
O throughput calculado é de 2,22 requisições por segundo (20 solicitações / 8,998 s). Esse valor reflete o mínimo de carga aplicada e pode ser significativamente inferior ao que a aplicação suportará em produção.

Latência  
A coluna “Latency” corresponde ao tempo até a primeira resposta. Os percentis foram calculados a partir dos 20 valores disponíveis.  
- p50: 48,5 ms  
- p90: 441 ms  
- p95: 816 ms  
- p99: 816 ms  

A maioria das requisições apresenta latência inferior a 100 ms. No entanto, duas chamadas apresentaram latências muito superiores (441 ms e 816 ms), elevando os percentis 90, 95 e 99. Esses valores indicam que, mesmo sob carga leve, existe possibilidade de picos de latência que podem degradar a experiência do usuário.

Taxa de erros  
Todas as respostas foram bem‑sucedidas (código 200 e flag success = true). A taxa de erros é 0 %. Entretanto, não foram verificadas as mensagens de resposta nem a validade do corpo, o que pode mascarar falhas de lógica de negócio.

Principais gargalos identificados  
1. Conexões de rede inconstantes – Os valores de “Connect” revelam dois tempos de conexão muito elevados (417 ms e 816 ms). Esses picos são responsáveis pelos altos valores de latência observados nos percentis 90/95/99.  
2. Baixa paralelização – Apenas duas threads foram utilizadas, o que limita drasticamente a carga e impede a detecção de gargalos de concorrência.  
3. Falta de análise de resposta – A ausência de assertivas sobre o conteúdo da resposta impede a identificação de erros lógicos que não geram falha HTTP.

Impacto potencial em ambiente de produção  
Em produção, a aplicação provavelmente enfrentará um número muito maior de usuários simultâneos. Se os gargalos de conexão persistirem, o tempo médio de resposta pode subir de forma não linear, causando gargalos de rede e aumentando a taxa de timeout em clientes. A baixa paralelização no teste também significa que o comportamento de escalonamento, concorrência de banco de dados e limites de conexões não foram avaliados. Assim, há risco de subdimensionamento de recursos, especialmente de rede e CPU, resultando em degradação de performance e possível queda de disponibilidade.

Recomendações técnicas claras e acionáveis  
1. **Aumentar a carga de teste** – Execute o JMeter com 50–100 threads e ramp-up de 30–60 s para simular um cenário mais próximo da produção.  
2. **Habilitar keep‑alive e pooling de conexões** – Configure o JMeter para reutilizar conexões TCP/SSL, reduzindo os tempos de “Connect” e a sobrecarga de handshake.  
3. **Melhorar o DNS caching** – Configure o sistema operacional ou JMeter para cachear resoluções DNS, eliminando latências de DNS que podem contribuir para os picos de conexão.  
4. **Adicionar assertions de validação** – Verifique o corpo da resposta, códigos de status e headers para garantir que não ocorram falhas lógicas mesmo com respostas 200.  
5. **Monitorar métricas de infraestrutura** – Coleta de CPU, memória, throughput de rede e tempo de resposta do servidor alvo. Utilize ferramentas como Grafana ou Prometheus para identificar pontos de contenção.  
6. **Isolar a API externa** – Caso o gargalo esteja na PokéAPI, considere implementar cache local (Redis ou memcached) ou um serviço de proxy que reduza chamadas externas.  
7. **Revisar a arquitetura de rede** – Se os tempos de conexão permanecerem altos, analise a rota de rede, possíveis firewalls, balanceadores de carga e políticas de QoS.  
8. **Implementar limites de taxa (rate limiting)** – Defina políticas no servidor para controlar o número de requisições simultâneas, evitando sobrecarga.  
9. **Documentar e repetir** – Registre os resultados em um banco de dados de desempenho e execute o teste periodicamente (por exemplo, cada release) para acompanhar regressões.  
10. **Planejar escalonamento horizontal** – Se a carga for maior que o throughput atual, adicione instâncias do serviço de backend e balanceador de carga para distribuir a carga.

Seguindo essas recomendações, será possível detectar e mitigar os gargalos identificados, garantindo que a aplicação mantenha desempenho aceitável sob cargas realistas em ambiente de produção.